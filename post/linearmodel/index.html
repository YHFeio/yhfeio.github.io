<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.69.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>机器学习：线性模型&nbsp;&ndash;&nbsp;YHFeio</title><link rel="stylesheet" href="/css/core.min.7a6dedeee7291c9daf16368afd3f5958f3793b2e6f9fa92597ff1df00f09a979724933f1b5bcf4264af992bb6fbee89c.css" integrity="sha384-em3t7ucpHJ2vFjaK/T9ZWPN5Oy5vn6kll/8d8A8JqXlySTPxtbz0Jkr5krtvvuic"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="机器学习：线性模型" /><body>
    <div class="base-body"><section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/logo.png" alt /><span class="site name">YHFeio</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/categories/">Categories</a><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/about/">About</a></nav></div></span></div><div class="site slogan"><span class="title">Nice Things</span></div></section><div id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">机器学习：线性模型</h1><p class="article date">2020-05-13</p></section><article class="article markdown-body"><h1 id="线性模型">线性模型</h1>
<h2 id="1-基本形式">1 基本形式</h2>
<p>由d个属性描述的示例 $$\pmb x = (x_1;x_2; \cdots ;x_d)$$，<code>线性模型(linear model)</code>试图学得一个通过属性的线性组合来进行预测的函数：<br>
$$f(\pmb x)=w_1x_1+w_2x_2+\cdots +w_dx_d + b$$<br>
向量形式：$$f(\pmb x)=\pmb w^T\pmb x+b$$，其中$$\pmb w=(w_1;w_2;\cdots;w_d)$$，<strong>w</strong> 和 b 学得之后，模型就得以确定。</p>
<ul>
<li>许多功能更强大的非线性模型(nonlinear model)可在线性模型的基础上通过引入层级结构或高维映射而得</li>
<li>w直观表达了各属性在预测中的重要性，模型具有很好的可解释性(comprehensibility)</li>
</ul>
<h2 id="2-线性回归">2 线性回归</h2>
<p>数据集$$D={(\pmb x_1,y_1),(\pmb x_2,y_2), \cdots , (\pmb x_m,y_m)}$$，其中$$\pmb x_i={x_{i1};x_{i2};\cdots;x_{id}},y_i \in R$$。<code>线性回归(linear regression)</code>试图学得一个线性模 型以尽可能准确地预测实值输出标记。</p>
<p>1）首先考虑输入属性的数目只有一个$$D={(x_i,y_i)_{i=1}^m}$$，对于离散属性，转化为连续值。线性回归试图学得:<br>
$$f(x_i)=wx_i + b,使得f(x_i)\approx y_i$$</p>
<p>让均方误差最小化来确定w、b：</p>
<p>$$
(w^*,b^*)  =\underset {(w,b)}{\operatorname {arg, min}}
\sum_{i=1}^{m}{(f(x_i)-y_i)^2} 
= \underset {(w,b)}{\operatorname {arg, min}} 
\sum_{i=1}^{m}{(y_i-wx_i-b)^2}
$$</p>
<p>均方误差的几何意义对应了<code>欧氏距离(Euclidean distance)</code>。基于均方误差最小化来进行模型求解的方法称为<code>最小二乘法(least square method)</code>，该方法在线性回归中试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。</p>
<p>求解 w 和 b 使 $$E_{(w,b)} = \sum_{i=1}^{m}{(y_i-wx_i-b)^2}$$最小化的过程，称为线性回归模型的最小二乘<code>参数估计(parameter estimation)</code>。将 E(w,b) 分别对 w 和 b 求导后为0，得到最优封闭(closed-form)解：<br>
$$w=\frac {\sum_{i=i}^{m}{y_i(x_i-\overline{x})}}
{\sum_{i=1}^{m}{x_i^2}-\frac{1}{m}(\sum_{i=i}^{m}{x_i})^2}$$<br>
$$b=\frac{1}{m} \sum_{i=i}{m}{(y_i-wx_i)}$$</p>
<p>2）更一般的样本由 d 个属性描述，这称为<code>多元线性回归(multivariate linear regression)</code>。<br>
同样使用最小二乘法估计，令$$\hat {\pmb w} = (\pmb w;b)$$，D表示为一个$$m\times (d+1)$$的矩阵<strong>X</strong>：每行对应于一个示例，行的前d个元素对应于d个属性值，最后一个为1。标记也写成向量形式：$$\pmb y=(y_1;y_2;\cdots;y_m)$$，则<br>
$$\hat {\pmb w}^* = 
\underset {\hat {\pmb w}}{\operatorname {arg min}}
{(\pmb{y-X\hat w})^T(\pmb{y-X\hat w})}$$</p>
<p>3）令模型预测值逼近y 的衍生物，如<code>对数线性回归(log-linear regression)</code>：<br>
$$\ln y = \pmb{w}^T\pmb{x}+b$$<br>
其在形式上仍是线性回归，但实质上已是在求取输入空间到输出空间的非线性函数映射。</p>
<p>更一般地，考虑单调可微函数 g(.)， 令：<br>
$$y=g^{-1}(\pmb{w}^T\pmb{x}+b)$$<br>
这样得到的模型称为<code>广义线性模型(generalized linear model)</code>，其中函数g(.)称为<code>联系函数(link function)</code>。（参数估计通常用加权最小二乘或极大似然估计）</p>
<h2 id="3-对数几率回归">3 对数几率回归</h2>
<p>对于分类任务，可利用广义线性回归：找一个单调可微函数将分类真实标记y与线性回归模型的预测值联系起来。</p>
<p>考虑二分类任务：$$y\in {{0,1}}$$，线性回归模型输出是实值，需要将实值z 转换为 0/1 值。理想的是<code>单位阶跃函数(unit-step function)</code> ，但该函数不连续，无法作为g-(.)。因此考虑<code>对数几率函数(logistic function)</code>：<br>
$$y=\frac {1}{1+e^{-z}}$$</p>
<p>是一种sigmoid函数，作为g-(.)带入广义线性模型：<br>
$$y = \frac {1}{1+e^{-(\pmb{w}^T\pmb{x}+b)}}$$<br>
可变形为：<br>
$$\ln {\frac {y}{1-y}} = \pmb{w}^T\pmb{x}+b$$</p>
<p>将y 视为样本 x 作为正例的可能性，则 1-y 是其反例可能性，两者的比值称为<code>几率(odds)</code>，反映了x作为正例的相对可能性。对几率取对数则得到<code>对数几率(log odds，亦称 logit)</code>。</p>
<p>实际上是用线性回归模型去逼近真实标记的对数几率，其对应的模型称为<code>对数几率回归(logistic regression，亦称 logit regression)</code>。</p>
<ul>
<li>直接对分类可能 性进行建模，无需事先假设数据分布</li>
<li>不是仅预测出&quot;类别&rdquo;，而是可得到近似概率预测</li>
<li>对率函数是任意阶可导的凸函数</li>
</ul>
<p><strong>参数估计</strong></p>
<div align=center><img src="/dmimg/logit.png" /></div>  
<h2 id="4-线性判别分析">4 线性判别分析</h2>
<p>线性判别分析(Linear Discriminant Analysis,LDA)，一种线性学习方法，思想：给定训练样例集，设法将样例投影到一条直线上，使同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；将新样本投影到这条直线上，根据投影点的位置来确定新样本的类别。</p>
<div align=center><img src="/dmimg/LDA.png" width = "60%" /></div>  
<p><strong>参数估计</strong></p>
<div align=center><img src="/dmimg/LDAguji.png" /></div>  
<h2 id="5-多分类学习">5 多分类学习</h2>
<p>一些二分类学习方法可以直接推广到多分类，更多情况下是基于一些基本策略，利用二分类学习器来解决多分类问题。<br>
基本思路是<code>拆解法</code>，即将多分类任务拆为若干个二分类任务求解。经典的拆分策略：</p>
<ul>
<li><code>一对一(One vs. One,OvO)</code></li>
<li><code>一对 其余(One vs. Rest,OvR)</code></li>
<li><code>多对多(Many vs. Many,MvM)</code></li>
</ul>
<p>OvO 将 N 个类别两两配对，产生 N(N-1)/2 个二分类任务。在测试阶段，新样本将同时提交给所有分类器，将得到 N(N -1)/2 个分类结果，最终结果为被预测得最多的类别。</p>
<p>OvR 每次将一个类的样例作为正例、所有其他类的样例作为反例来训练 N 个分类器。测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果，若有多个分类器预测为正类， 则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果。</p>
<p>OvO训练的分类器更多，存储开销和测试时间开销通常比 OvR 更大；OvO每个分类器仅用到两个类的样例进行训练，OvR均使用全部训练样例，在类别很多时，OvO 的训练时间开销通常比 OvR 更小；预测性能取决于具体的数据分布，在多数情形下两者差不多。</p>
<p>MvM 每次将若干个类作为正类，若干个其他类作为反类。最常用的 MvM 技术：<code>纠错输出码(Error Correcting Output Codes,ECOC)</code>，将编码的思想引入类别拆分，并尽 可能在解码过程中具有容错性。</p>
<ul>
<li><code>编码</code>:将N个类别分成正反两类形成二分类训练集，共划分 M 次产生 M 个训练集，可训练出 M 个分类器。</li>
<li><code>解码</code>:M 个分类器的预测标记组成一个编码，与每个类别各自的编码进行比较，返回其中距离最小 的类别作为最终预测结果.</li>
</ul>
<p>类别划分通过<code>编码矩阵(coding matrix)</code>指定，常见的有二元码（正、反类）和三元码（正、反和停用类），示意图：</p>
<div align=center><img src="/dmimg/bianma.png" width = "80%"/></div>
<p>称为纠错输出码是因为在测试阶段，假如某个分类器出错了，基于产生的编码仍有很大概率产生正确的最终分类结果，ECOC 编码越长，纠错能力越强。但编码越长，意味着所需训练的分类器越多。<br>
理论上任意两个类别之间的编码距离越远，则纠错能力越强，码长较小时可根据这个原则计算出理论最优编码，但码长稍大一些就难以有效地确定最优编码，事实上这是 NP 难问题。不过，通常非最优编码在实践中往往己能产生足够好的分类器。</p>
<h2 id="5-类别不平衡问题">5 类别不平衡问题</h2>
<p><code>类别不平衡(class-imbalance)</code>是指分类任务中不同类别的训练样例数目差别很大。假设正例较少、反例较多。</p>
<p>用对数几率进行分类时，事实上是在用预测出的 y 值与一个阈值进行比较，通常是0.5，大于为正例，即分类器决策规则为：<br>
$$若\frac{y}{1+y} &gt; 1，则预测为正例$$</p>
<p>但当训练集中正、反例的数目不同时，令m+和m-分别表示正、反例数目，则观测几率是$$\frac{m^+}{m^-}$$，通常假设训练集是真实样本总体的无偏采样，因此观测几率就代表了真实几率。因此，只要分类器的预测几率高于观测几率就应判定为正例：</p>
<p>$$若\frac{y}{1+y} &gt; \frac{m^+}{m^-}，则预测为正例$$</p>
<p>令：<br>
$$\frac{y&rsquo;}{1+y&rsquo;} = \frac{y}{1+y} \times \frac{m^+}{m^-}$$</p>
<p>这就是类别不平衡学习的一个基本策略——<code>再缩放(rescaling)</code>。</p>
<p>“训练集是真实 样本总体的无偏采样”这个假设往往并不成立，我们未必能有效地基于训练集观测几率来推断出真实几率。</p>
<p>现有技术大体上有三类做法：</p>
<ul>
<li>直接对训练集里的反类样例进行<code>欠采样(undersampling)</code>，即去除一些反倒使得正、反例数目接近，然后再进行学习。欠采样法若随机丢弃反例，可能丢失一些重要信息。代表性算法是利用集成学习机制，将反倒划分为若干个 集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来 看却不会丢失重要信息。</li>
<li>对训练集里的 正类样例进行<code>过来样(oversampling)</code>，即增加一些正例使得正、反例数目 接近，然后再进行学习。不能简单地对初始正例样本进行重 复采样，否则会招致严重的过拟合。代表性算法是通过对训练集里的正例进行插值来产生额外的正例。</li>
<li>直接基于原始训练集进行学习，但在用 训练好的分类器进行预测时，将上式嵌入到其决策过程中，称为<code>阈值移动(threshold-moving)</code>。</li>
</ul>
</article><section class="article labels"><a class="category" href=/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>数据挖掘_机器学习</a></section></div><section class="article navigation"><p><a class="link" href="/post/decisiontree/"><span class="li">&larr;</span>机器学习：决策树</a></p><p><a class="link" href="/post/modeleas/"><span class="li">&rarr;</span>机器学习：模型评估与选择</a></p></section></div><section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 Notepadium.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div></section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>