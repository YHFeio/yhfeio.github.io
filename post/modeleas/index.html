<!DOCTYPE html>
<html lang="en"><meta charset="utf-8"><meta name="generator" content="Hugo 0.69.0" /><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>机器学习：模型评估与选择&nbsp;&ndash;&nbsp;YHFeio</title><link rel="stylesheet" href="/css/core.min.7a6dedeee7291c9daf16368afd3f5958f3793b2e6f9fa92597ff1df00f09a979724933f1b5bcf4264af992bb6fbee89c.css" integrity="sha384-em3t7ucpHJ2vFjaK/T9ZWPN5Oy5vn6kll/8d8A8JqXlySTPxtbz0Jkr5krtvvuic"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="机器学习：模型评估与选择" /><body>
    <div class="base-body"><section id="header" class="site header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><img class="site logo" src="/logo.png" alt /><span class="site name">YHFeio</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/categories/">Categories</a><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/about/">About</a></nav></div></span></div><div class="site slogan"><span class="title">Nice Things</span></div></section><div id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">机器学习：模型评估与选择</h1><p class="article date">2020-05-12</p></section><article class="article markdown-body"><h1 id="模型评估与选择">模型评估与选择</h1>
<h2 id="1-经验误差与过拟合">1 经验误差与过拟合</h2>
<p>1、<code>错误率(error rate)</code>：分类错误的样本数占样本总数的比例。m个样本中有a个样本分类错误，错误率E=a/m。<br>
2、<code>精度(accuracy)</code>：1-a/m，即分类正确的样本比例。<br>
3、<code>误差(error)</code>：学习器的实际预测输出与样本的真实输出之间的差异。<br>
4、<code>训练误差(training error)/经验误差(empirical error)</code>：学习器在训练集上的误差。<br>
5、<code>泛化误差</code>(generalization error)`：在新样本上的误差。</p>
<p>我们希望得到泛化误差小的学习器，但新样本事先是不知道的，因此要使经验误差最小化。</p>
<p><code>过拟合(overfitting)</code>：学习器过度学习，把训练样本自身的一些特点当作所有潜在样本都会具有的一般性质，训练集上表现很好，但泛化性能差。<br>
<code>欠拟合(underfitting)</code>：指对训练样本的一般性质尚未学好。<br>
欠拟合容易克服，过拟合无法彻底避免。（NP难问题，有效学习算法在多项式时间内完成，若过拟合可彻底避免，则经验误差最小化可得到最优解，证明“P=NP”，&ldquo;P!=NP&rdquo;-&gt;无法避免）。</p>
<p><code>模型选择(model selection)</code>：学习算法及其参数配置的选择。</p>
<h2 id="2-评估方法">2 评估方法</h2>
<p>通过实验评估学习器的泛化误差，这就需要<code>测试集(testing set)</code>测试学习器对新样本的判别能力，测试集上的<code>测试误差(testing error)</code>作为泛化误差的<code>近似</code>。</p>
<ul>
<li>假定测试集是从样本真实分布中独立同分布采样而得。</li>
<li>测试集应该尽可能与训练集互斥。</li>
</ul>
<p>从包含m个样例的数据集$$D={(x_1,y_1),(x_2,y_2), \cdots , (x_m,y_m)}$$ 中划分训练集S和测试集T的常用方法。</p>
<h3 id="21-留出法">2.1 留出法</h3>
<p><code>留出法(hold-out)</code>：直接将数据集 D 划分为两个互斥的集合，分别作为S和T，在 S上训练出模型后，用T评估其测试误差作为对泛化误差的估计。</p>
<ul>
<li>训练/测试集的划分要尽可能保持数据分布的一致性，可采用<code>分层采样(stratified sampling)</code>。</li>
<li>确定训练/测试样本比例后，仍有许多划分方式获得不同的S、T，不同的模型评估结果。采用若干次随机划分、重复实验评估后取平均值。</li>
<li>S大、T小，接近整个D训练出的模型，但评估结果不稳定；S小、T大，与D训练出的模型差别大，降低评估结果保真性(fidelity)。常用 2/3 或 4/5 的 样本用于训练。</li>
</ul>
<h3 id="22-交叉验证法">2.2 交叉验证法</h3>
<p><code>交叉验证法(cross validation)</code>将数据集 D 划分为 k 个大小相似的互斥子集，每个子集 $$D_i$$ 都尽可能保持数据分布的一致性。每次用 k-1 个子集的并集作为训练集，余下的那个子集作为测试集。有 k 组训练/测试集可进行 k 次实验，最终返回k个测试结果的均值。</p>
<p>交叉验证法评估结果的稳定性和保真性在很大程度上取决于 k 的取值，通常又称为<code>k折交叉验证(k-fold cross validation)</code>。K常取10、5、20。</p>
<p>类似留出法，k折交叉验证通常要随机使用不同的划分重复 p 次，最终的评估结果是这 p 次 k 折交叉验证结果的均值。</p>
<p><code>留一法(Leave-One-Out，简称 LOO)</code>，交叉验证法的一 个特例，令k等于D中的样本数。</p>
<ul>
<li>不受随机样本划分的影响，只有一种划分方法。</li>
<li>被实际评估的模型与期望评估的用 D 训练出的模型很相似。</li>
<li>但在数据集比较大时，训练 m 个模型的计算开销可能是难以忍受的。</li>
<li>估计结果也未必永远比其他评估方法准确，<code>没有免费的午餐</code>定理。</li>
</ul>
<h3 id="23-自助法">2.3 自助法</h3>
<p>留出法和交叉验证法保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，会导致估计偏差。.留一法计算复杂度高。<br>
<code>自助法(bootstrapping)</code>直接以自助采样法(bootstrap sampling)为基础。从样本量为m的数据集D中采样生成D&rsquo;：有放回地抽m次。样本始终不被采到的概率是$$(1-\frac{1}{m})^m$$，m取无穷极限得0.368。将D'用作训练集， D\D'用作测试集。这样的测试结果，亦称<code>包外估计(out-of-bag estimate)</code>。</p>
<ul>
<li>在数据集较小、难以有效划分训练/测试集时很有用</li>
<li>能从初始数据集中产生多个不同的训练集，对集成学习等方法有很大的好处</li>
<li>改变了初始数据集的分布，这会引入估计偏差</li>
<li>在初始数据量足够时，留出法和交叉验证法更常用一些</li>
</ul>
<h3 id="24-调参与最终模型">2.4 调参与最终模型</h3>
<p>学习算法的参数（parameter）设置不同，学得的模型性能差别明显，参数调节也称<code>调参(parameter tuning)</code>，即对算法参数进行设定。</p>
<ul>
<li>参数取值一般为实数集，一般选定一个范围和变化步长来分别进行评估。</li>
<li>测试集用于估计实际使用的泛化性能，训练数据另外划分为训练集和<code>验证集(validation set)</code>，基于验证集上的性能进行模型选择和调参。</li>
<li>在确定了模型和参数后，需要应用D重新训练模型作为最终模型。</li>
</ul>
<h2 id="3-性能度量">3 性能度量</h2>
<p>上节为评估学习器泛化性能的实验估计方法，还需要衡量模型泛化能力的评价标准，即<code>性能度量(performance measure)</code>。</p>
<p>性能度量反映了任务需求，使用不同的性能度量会导致不同的评判结果。<br>
预测任务中给定$$D={(x_1,y_1),(x_2,y_2), \cdots , (x_m,y_m)}$$，其中y是x的真实标记，学习器f，学习器预测结果f(x)。</p>
<p>回归任务常用<code>均方误差(mean squared error)</code>：<br>
$$ E(f;D)=\frac{1}{m} \sum_{i=1}^{m}{(f(x_i)-y_i)^2}$$</p>
<p>更一般的，对于数据分布D和概率密度函数p(.)，均方误差可描述为：
$$E(f;D)=\int_{x~D}{(f(x)-y)^2 p(x)dx}$$</p>
<p>下面主要介绍分类任务中常用的性能度量。</p>
<h3 id="31-错误率与精度">3.1 错误率与精度</h3>
<p>同开头定义的一样，即分类错误/正确的样本占总样本比例，更一般地对于数据分布D和概率密度函数p(.)：<br>
$$ 错误率：E(f;D)=\int_{x~D}{Ⅱ(f(x) \neq y) p(x)dx}$$<br>
$$ 精度：acc(f;D)=\int_{x~D}{Ⅱ(f(x) = y) p(x)dx}$$<br>
Ⅱ()中表达式成立为1，不成立为0.</p>
<h3 id="32-查准率查全率与fl">3.2 查准率、查全率与Fl</h3>
<p>二分类问题，可将样例根据其真实类别与学习器预测类别的组合划 分为<code>真正例(true positive)</code>、<code>假正例(false positive)</code>、<code>真反例(true negative)</code>、 <code>假反例(false negative)</code>四种情形。 TP、FP、TN、FN对应各自样例数，则它们的和为样例总数。<code>混淆矩阵矩(confusion matrix)</code>定义为：</p>
<table>
<thead>
<tr>
<th>真实情况</th>
<th>预测结果</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>正例</td>
<td>反例</td>
</tr>
<tr>
<td>正例</td>
<td>TP(真正例)</td>
<td>FN(假反例)</td>
</tr>
<tr>
<td>反例</td>
<td>FP(假正例)</td>
<td>TN(真反例)</td>
</tr>
</tbody>
</table>
<p>1）<code>查准率(precision)</code>P表示预测为正例的样例中有多少是真正的正例。<code>查全率(recall)</code> R表示所有正例有多少被正确的预测出来。它们分别定义为：<br>
$$P=\frac {TP}{TP+FP}$$<br>
$$R=\frac {TP}{TP+FN}$$</p>
<p>查准率和查全率是一对矛盾的度量，P高时，R偏低，R高时，P偏低。</p>
<p>2）<code>P-R曲线</code>：可以根据学习器的预测结果（如输出是正例的概率）对样例进行排序，按照学习器认为是正例的样本的可能性从大到小排。然后按此顺序逐个把样本预测为正例，则每次可以计算出当前的查全率、 查准率。以查准率为纵轴、查全率为横轴作图，即查准率-查全率曲线简称&quot;P-R曲线&rdquo;。如</p>
<div align=center><img src="/dmimg/PR.png" width = "60%" /></div>
<p>若一个学习器的 P-R 曲线被另一个完全&quot;包住&rdquo;，则后者的性能优于前者（AB优于C），发生交叉则不好判断。其他度量：</p>
<ul>
<li><code>平衡点(Break-Event Point, BEP)</code>是<em>查准率=查全率</em>时的取值。</li>
<li>BEP过于简化，更常用的是 F1 度量:$$F1=\frac {2 \times P \times R}{P+R}=\frac {2 \times TP}{样例总数+TP-TN}$$</li>
<li>对查准率和查全率的重视程度不同，考虑 F1 度量的一般形式$$F_{\beta}=\frac{(1+\beta^2) \times P \times R}{(\beta^2 \times P) + R}$$；其中$$\beta &gt;0$$，等于1时为F1，大于1查全率有更大影响，小于1查准率有更大影响。</li>
</ul>
<p>3）在 n 个二分类混淆矩阵上综合考察查准率和查全率。</p>
<ul>
<li>直接的方法时各混淆矩阵上分别计算出查准率和查全率，再求平均得到<code>宏查准率(macro-P)</code>、<code>宏查全率(macro-R)</code>，以及相应的<code>宏F1(macro-F1)</code>。</li>
<li>将各混淆矩阵的对应元素进行平均，得到 TP、 FP、 TN、 FN 的 平均值，再基于这些平均值计算出<code>微查准率(micro-P)</code>、<code>微查全率(micro-R)</code>和<code>微F1(micro-F1)</code>。</li>
</ul>
<h3 id="33-roc-与-auc">3.3 ROC 与 AUC</h3>
<p>很多学习器是为测试样本产生一个实值或概率预测，然后同设定的分类阈值(threshold)比较，若大于阈值则分为正类，否则为反类。</p>
<p><code>ROC</code>全称是<code>受试者工作特征(Receiver Operating Characteristic)曲线</code>，与P-R 曲线相似，排序后逐个把样本作为正例预测，然后计算两个值，不同的是ROC 曲线的纵轴是<code>真正例率(True Positive Rate, TPR)</code>，横轴是<code>假正例率(False Positive Rate,FPR)</code>，分别定义为：
$$TPR=\frac {TP}{TP+FN}; FPR=\frac {FP}{TN+FP}$$<br>
真正例率和查全率相等，是所有正例中被预测为正的比例，假正例率是所有反例中被预测为正的比例。roc示意图：</p>
<div align=center><img src="/dmimg/roc.png" width = "80%" /></div>
<p>现实中通过有限的样例获取有限个（FPR,TPR）坐标对，因此无法产生左边光滑的曲线，只能绘制出右图近似ROC曲线，绘制过程：给定$$m^+和m^-$$个正反例</p>
<ul>
<li>据学习器预测结果对样例进行排序</li>
<li>首先所有样本预测为反例，TPR和FPR均为0</li>
<li>依次将每个样例划分为正例（看作是阈值从最大变化到最小），设前一个标记点坐标为 (x,y)，当前若为真正例，则对应标记点的坐标为$$(x,y+\frac{1}{m^+})$$;当前若为假正例，则对应标记点的坐标为$$(x+\frac{1}{m^-},y)$$</li>
<li>线段连接相邻点</li>
</ul>
<p>若一个学习器的 ROC 曲线被另一个曲线完全&quot;包住&rdquo;，后者的性能优于前者。若存在交叉，可比较 ROC 曲线下的面积<code>AUC(Area Under ROC Curve)</code>。假定ROC由坐标{(x1,y1),(x2, y2), &hellip; , (xm, ym)}构成，则<br>
$$AUC=\frac{1}{2} \sum_{i=1}^{m-1}{(x_{i+1}-x_i) \cdot (y_i + y_{i+1})}$$</p>
<h3 id="34-代价敏感错误率与代价曲线">3.4 代价敏感错误率与代价曲线</h3>
<p>现实中反例预测为正和正例预测为反的错误的后果不同，可为错误赋予<code>非均等代价(unequa1 cost)</code>。二分类任务中可根据领域知识设定一个<code>代价矩阵(cost matrix)</code>，costij表示将第 i 类样本预测为第 j 类 样本的代价：</p>
<table>
<thead>
<tr>
<th>真实类别</th>
<th>预测类别</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>第0类</td>
<td>第1类</td>
</tr>
<tr>
<td>第0类</td>
<td>0</td>
<td>cost01</td>
</tr>
<tr>
<td>第1类</td>
<td>cost10</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>之前大都隐式地假设了均等代价，若第0类作为正类、第1类作为反类，令D+与D-分别代表正例子集和反例子集，则<code>代价敏感(cost-sensitive)错误率</code>为：</p>
<p>$$
E(f;D;cost)=\frac {1}{m} 
(\sum_{x_i \in D^+} Ⅱ(f(x_i) \neq y_i) 
\times cost_{01} + \<br>
\sum_{x_i \in D^-} Ⅱ(f(x_i) \neq y_i) \times cost_{10})$$</p>
<p>在非均等代价下ROC曲线不能直接反映学习器的期望总体代价，可用<code>代价曲线(cost curve)</code>，横轴是取值为 [0,1] 的正例概率代价：</p>
<p>$$P(+)cost=\frac {p \times cost_{01}} {p \times cost_{01}+(1-p)\times cost_{10}}$$</p>
<p>其中 p 是样例为正例的概率;纵轴是取值为 [0,1] 的归一化代价：<br>
$$
cost_{norm}=\frac{FNR\times p \times cost_{01}+FPR \times (1-p)\times cost_{10}}
{p \times cost_{01}+(1-p)\times cost_{10}}
$$<br>
FPR是假正例率，FNR =1 - TPR 是假反例率</p>
<h2 id="4-比较检验">4 比较检验</h2>
<p>上面介绍了实验评估方法和性能度量，可使用某种实验评估方法测得学习器的某个性能度量结果，然后对这些结果进行比较。但这里涉及几个重要因素:</p>
<ul>
<li>希望比较的是泛化性能，而获得的是测试集上的性能。</li>
<li>测试集上的性能与测试集本身的选择有很大关系，不同测试集测试结果不同。</li>
<li>很多机器学习算法本身有一定的随机性，即便用相同的参数设置在同一个测试集上多次运行，其结果也会有不同。</li>
</ul>
<p>统计假设检验(hypothesis test)为我们进行学习器性能比较提供了重要依据，本节默认以错误率为性能度量，用e表示。</p>
<h3 id="41-假设检验">4.1 假设检验</h3>
<p>在包含 m 个样本的测试集上， 泛化错误率为 e 的学习器被测得测试错误率为 $$\hat e$$的概率:</p>
<p>$$P(\hat e;e) = \begin{pmatrix} m \ \hat e \times m\ \end{pmatrix} e^{\hat e \times m}(1-e)^{m- \hat e \times m}$$</p>
<p>则解 $$\partial P(\hat e; e)/\partial e = 0$$可知，P在$$e= \hat e$$时最大， $$|e-\hat e|$$增大时P减小。这符合二项(binomial)分布。</p>
<p>1）使用<code>二项检验(binomial test)</code>进行假设检验，更一般的，考虑假设 $$e \leq e_0$$，则在 1-α 的概率（置信度(confidence)）内所能观测到的最大错误率：
$$\overline e = max e \quad s.t.\quad
\sum_{i=e_0 \times m+1}^{m}{
\begin{pmatrix} m \ i \ \end{pmatrix}
e^i(1-e)^{m-i} &lt; \alpha
}
$$</p>
<p>若测试错误率$$\hat e$$小于临界值$$\overline e$$，则在α的显著度下，假设不能被拒绝，即泛化错误率不大于e0。</p>
<p>2）t检验</p>
<p>多次留出法或交叉验证得到多个错误率$$\hat{e_1},\hat{e_2}, \cdots, \hat{e_k}$$，计算它们的均值方差分别为$$\mu,\sigma^2$$，注计算方差时比k-1。k个测试错误率可看作泛化错误率e0的独立采样，则：
$$\tau_t=\frac{\sqrt{k}(\mu - e_0)}{\sigma}$$</p>
<p>服从自由度为 k-l 的 t 分布。对假设$$\mu =e_0$$和显著度 α，考虑双边检验，若$$\mu -e_0$$位于$$[t_{-\alpha/2},t_{\alpha /2}]$$内，则不拒绝原假设。</p>
<p>上述两种方法是对单个学习器泛化性能的假设检验，下面介绍对不同学习器性能的比较时的假设检验。</p>
<h3 id="42-交叉验证t检验">4.2 交叉验证t检验</h3>
<p>对学习器A,B，用k折交叉验证法分别测得泛化误差$$e_1^A,e_2^A \cdots e_k^A;e_1^B,e_2^B \cdots e_k^B$$。</p>
<ul>
<li>$$e_i^A和e_i^B $$是在相同第i折训练/测试集得到的结果。</li>
<li>若AB性能相同，则$$\Delta_i=e_i^A-e_i^B$$应该为0。</li>
<li>计算$$\Delta_i$$的均值和方差$$\mu,\sigma^2$$。</li>
</ul>
<p>显著度α下，若<br>
$$\tau_t=|\frac{\sqrt{k}\mu}{\sigma}|$$<br>
小于临界值$$t_{\alpha/2,k-1}$$，则假设不能被拒绝，即认为两个学习器的性能没有显著差别，否则平均错误率较小的那个学习器性能较优。</p>
<p>不同轮次训练集样本存在重叠，使得测试错误率实际上并不独立，会导致过高估计假设成立的概率。采用“5x2交叉验证”，即做5 次 2 折交叉验证，且在每次 2 折交叉验证之前随机将数据打乱。对于AB两个学习器</p>
<ul>
<li>第i次2折交叉验证将产生两对测试错误率，我们对它们分别求差，得到第 1 折上的差值$$\Delta_i^1$$和第 2 折上的差值$$\Delta_i^2$$。</li>
<li>计算$$\mu=0.5(\Delta_1^1 + \Delta_1^2)$$，以及$$\sigma_i^2=(\Delta_i^1-\frac {\Delta_i^1 + \Delta_i^2}{2})^2+(\Delta_i^2-\frac {\Delta_i^1 + \Delta_i^2}{2})^2$$。</li>
<li>变量$$\tau_t=\frac{\mu} {\sqrt{0.2\sum_{i=1}^{5}{\sigma_i^2}}}$$ 服从自由度为 5 的 t 分布，其双边检验的临界值$$t_{\alpha/2,5}$$当α=0.05 时为 2.5706， α= 0.1 时为 2.0150。</li>
</ul>
<h3 id="43-mcnemar-检验">4.3 McNemar 检验</h3>
<h3 id="44-friedman-检验与-nemenyi-后续检验">4.4 Friedman 检验与 Nemenyi 后续检验</h3>
<h2 id="5-偏差与方差">5 偏差与方差</h2>
<p><code>偏差-方差分解(bias-variance decomposition)</code>解释学习算法泛化性能的一种重要工具。对学习算法的期望泛化错误率进行拆解。</p>
<p>假定测试样本x，yD为x在数据集中的标记（含有噪声），y为x的真实标记，f(x;D)为训练集D上学得的模型f在x上的预测输出，以回归为例，</p>
<ul>
<li>学习算法的期望预测为：$$\overline f(x)=E_D[f(x;D)]$$</li>
<li>使用样本数相同的不同训练集产生的方差为：$$var(x)=E_D[(f(x;D)-\overline f(x)^2]$$</li>
<li>噪声为：$$\varepsilon^2=E_D[(y_D-y)^2]$$</li>
<li>期望输出与真实标记的差别称为偏差(bias)：$$bias^2(x)=(\overline f(x)-y)^2$$</li>
</ul>
<p>假定噪声期望为零，即 $$E_D[y_D -y] =0$$，对算法的期望泛化误差进行分解得:<br>
$$E(f;D) = bias^2(x)+var(x) + \varepsilon^2$$<br>
即泛化误差可分解为偏差、方差与噪声之和。偏差一方差分解说明，泛化性能是由 学习算法的能力、数据的充分性以及学习任务本身的难度所共同决定的。</p>
<p>偏差与方差是有冲突的，这称为偏差一方差窘境(bias-variance dilemma)。给定学习任务，在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以便学习器产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化错误率；在训练程度充足后，学习器的拟合能力已非常强，训练 数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的、非全 局的特性被学习器学到了，则将发生过拟合。</p>
</article><section class="article labels"><a class="category" href=/categories/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>数据挖掘_机器学习</a></section></div><section class="article navigation"><p><a class="link" href="/post/linearmodel/"><span class="li">&larr;</span>机器学习：线性模型</a></p><p><a class="link" href="/post/dm1/"><span class="li">&rarr;</span>数据挖掘：概念、数据</a></p></section></div><section id="footer" class="footer"><div class="footer-wrap">
    <p class="copyright">©2020 Notepadium.</p>
    <p class="powerby"><span>Powered by </span><a href="https://gohugo.io" 
        target="_blank">Hugo</a><span> and the </span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank">Notepadium</a></p>
</div></section><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/&#43;DiW/UqRcLbRjq" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l&#43;B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd&#43;qj&#43;o24G5ZU2zJz" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script></div>
</body>

</html>